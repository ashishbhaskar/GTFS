{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.transit import gtfs_realtime_pb2\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from zipfile import ZipFile\n",
    "from pyproj import Geod\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import requests\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from protobuf_to_dict import protobuf_to_dict\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io import sql\n",
    "from IPython.display import clear_output\n",
    "import pika\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgs84_geod = Geod(ellps='WGS84') #Distance will be measured on this ellipsoid - more accurate than a spherical method\n",
    "def monthname(mydate):\n",
    "    mydate = datetime.datetime.now()\n",
    "    m = mydate.strftime(\"%B\")\n",
    "    return(m)\n",
    "\n",
    "def Brisbane(epoch):\n",
    "    a = time.strftime(\"%d-%m-%Y %H:%M:%S\", time.localtime(epoch))\n",
    "    return(a)\n",
    "\n",
    "def createCSVfile(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            k = 0\n",
    "            for item in i:  \n",
    "                f.write(str(item) + ',')\n",
    "                k = k+1\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def inputCSVfile(csvfile):\n",
    "    list1= []\n",
    "    with open(csvfile, 'r') as f:\n",
    "        for i in f:\n",
    "            j = i.split(',')\n",
    "            \n",
    "            le = len(j)\n",
    "            j[le - 1] = (j[le- 1]).strip()\n",
    "            list1.append(j)\n",
    "        return(list1)\n",
    "\n",
    "def createCSVfileWCD(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            f.write(str(i))\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def Distance(lat1,lon1,lat2,lon2):\n",
    "    az12 ,az21,dist = wgs84_geod.inv(lon1,lat1,lon2,lat2) #Yes, this order is correct\n",
    "    return (dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = r\"Y:\\Data\\GTFS_NEW/\"   #ensure slash sign in the end \"/\" not \"\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Static_Folder = Working_Directory + 'GTFS Static/'\n",
    "Realtime_folder  = Working_Directory + 'GTFS Realtime/'\n",
    "if not os.path.exists(Static_Folder):\n",
    "    os.makedirs(Static_Folder)\n",
    "if not os.path.exists(Realtime_folder):\n",
    "    os.makedirs(Realtime_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_static_link = r\"https://gtfsrt.api.translink.com.au/GTFS/SEQ_GTFS.zip\"\n",
    "gtfs_realtime_link = r'https://gtfsrt.api.translink.com.au/api/realtime/SEQ/TripUpdates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0\n",
    "datee = datetime.datetime.strptime(str(Brisbane(t1)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "m = datee.month\n",
    "y = datee.year\n",
    "d = datee.day\n",
    "m_name = monthname(Brisbane(t1))\n",
    "date_name1 = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "TableName = str(m_name)+\"_\"+str(y)\n",
    "TableName = TableName.lower()\n",
    "while True:\n",
    "    try:\n",
    "        clear_output(wait=True)\n",
    "        #Directories for a new Day\n",
    "        t = int(time.time())\n",
    "        datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "        m = datee.month\n",
    "        y = datee.year\n",
    "        d = datee.day\n",
    "        m_name = monthname(Brisbane(t))\n",
    "        date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "        Month_year = str(m_name)+\" ,\"+str(y)\n",
    "        if date_name!= date_name1:\n",
    "            index_o = 0\n",
    "            datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "            m = datee.month\n",
    "            y = datee.year\n",
    "            d = datee.day\n",
    "            m_name = monthname(Brisbane(t))\n",
    "            date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "            Month_year = str(m_name)+\" ,\"+str(y)\n",
    "            TableName = str(m_name)+\"_\"+str(y)\n",
    "            TableName = TableName.lower()\n",
    "            TUdirectory = Realtime_folder + \"TripUpdate entity\" +\"/\" + 'TU '+str(Month_year)\n",
    "            VPdirectory = Realtime_folder + \"VehiclePosition entity\" +\"/\" + 'VP '+str(Month_year)\n",
    "            if not os.path.exists(TUdirectory):\n",
    "                os.makedirs(TUdirectory)\n",
    "            TUdirectory2= TUdirectory+ \"/\"+ 'TU '+str(date_name)\n",
    "            if not os.path.exists(TUdirectory2):\n",
    "                os.makedirs(TUdirectory2) \n",
    "            TUdirectory3= TUdirectory2+ \"/\"+ 'TU feeds '+str(date_name)\n",
    "            TUdirectory4= TUdirectory2+ \"/\"+ 'TU Speed Analysis'+str(date_name)\n",
    "            if not os.path.exists(TUdirectory3):\n",
    "                os.makedirs(TUdirectory3)\n",
    "            if not os.path.exists(TUdirectory4):\n",
    "                os.makedirs(TUdirectory4)\n",
    "            if not os.path.exists(VPdirectory):\n",
    "                os.makedirs(VPdirectory)\n",
    "            VPdirectory2= VPdirectory+ \"/\"+ 'VP '+str(date_name)\n",
    "            if not os.path.exists(VPdirectory2):\n",
    "                os.makedirs(VPdirectory2)\n",
    "#Downloading and processing static file\n",
    "\n",
    "            GTFS_Static = urllib.request.urlretrieve(gtfs_static_link, Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "            zip_file = ZipFile(Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "            trips = pd.read_csv(zip_file.open('trips.txt'))\n",
    "            stop_times = pd.read_csv(zip_file.open('stop_times.txt'))\n",
    "            shapes = pd.read_csv(zip_file.open('shapes.txt'))\n",
    "            shapes['shape_pt_sequence'] = shapes['shape_pt_sequence'].astype(str)\n",
    "            Route_Shape_stop = stop_times.merge(trips, on = 'trip_id', how = 'left')\n",
    "            Route_Shape_stop = Route_Shape_stop[['shape_id', 'stop_id', 'stop_sequence', 'route_id', 'trip_id']].drop_duplicates(keep = 'first')\n",
    "            Route_Shape_stop['stop_sequence'] = Route_Shape_stop['stop_sequence'].astype(int)\n",
    "            Route_Shape_stop['stop_id'] = Route_Shape_stop['stop_id'].astype(str)\n",
    "            shapes2 = shapes.copy(deep = True)\n",
    "            shapes2['shape_pt_sequence_next'] = (shapes2['shape_pt_sequence'].astype(int) + 1).astype(str)\n",
    "            shapes2['stop_seq_1'] = (shapes2['shape_pt_sequence'].astype(int)/10000).astype(int)\n",
    "            shapes2['stop_seq_2'] = (shapes2['shape_pt_sequence_next'].astype(int)/10000).astype(int)\n",
    "            shapes3 = shapes2.merge(shapes, left_on = ['shape_id', 'shape_pt_sequence_next'], right_on = ['shape_id', 'shape_pt_sequence'], how = 'left')\n",
    "            shapes3['distance'] = Distance(shapes3['shape_pt_lat_x'].tolist(),shapes3['shape_pt_lon_x'].tolist(),shapes3['shape_pt_lat_y'].tolist(),shapes3['shape_pt_lon_y'].tolist())\n",
    "            shapes3['distance'] = shapes3['distance']/1000\n",
    "            shapes3 = shapes3.loc[shapes3['stop_seq_1'] == shapes3['stop_seq_2']]\n",
    "            shapes3['stop_seq_2'] = shapes3['stop_seq_2'] + 1\n",
    "            shapes4 = pd.DataFrame(shapes3.groupby(['shape_id', 'stop_seq_1', 'stop_seq_2'], as_index = False)['distance'].sum())\n",
    "            distance_file = shapes4\n",
    "            date_name1 = date_name\n",
    "#             distance_file.to_csv('distance_file.csv')\n",
    "        \n",
    "        feed = gtfs_realtime_pb2.FeedMessage()\n",
    "        response = urllib.request.urlopen(gtfs_realtime_link)\n",
    "        feed.ParseFromString(response.read())\n",
    "        dict_obj = protobuf_to_dict(feed)\n",
    "        r = json.dumps(dict_obj)\n",
    "        loaded_r = json.loads(r)\n",
    "        kppa = pd.DataFrame(loaded_r['entity'])\n",
    "        kt = kppa.trip_update.apply(pd.Series)\n",
    "        kt = pd.concat([kppa, kt], axis= 1)\n",
    "        kt2 = kt['stop_time_update'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\n",
    "        kt2 = kt.merge(kt2, left_index=True, right_index=True, how = 'left')\n",
    "        kt2 = kt2[[ 'id', 'trip_update', 'vehicle',  'stop_time_update', 'timestamp', 'trip', 'value']]\n",
    "        kt3 = kt2.value.apply(pd.Series)\n",
    "        kt3_1 = kt3.arrival.apply(pd.Series)\n",
    "        kt3_1 = kt3_1[['delay', 'time', 'uncertainty']]\n",
    "        kt3_1.columns = [ 'arrival_delay', 'arrival_time', 'arrival_uncertainty']\n",
    "        kt3_2 = kt3.departure.apply(pd.Series)\n",
    "        kt3_2 = kt3_2[['delay', 'time', 'uncertainty']]\n",
    "        kt3_2.columns = ['departure_delay', 'departure_time', 'departure_uncertainty']\n",
    "        kt3_f = pd.concat([kt3, kt3_1,kt3_2], axis = 1)\n",
    "        kt3_f = kt3_f[[ 'schedule_relationship', 'stop_id', 'stop_sequence',\n",
    "                              'arrival_delay', 'arrival_time', 'arrival_uncertainty', 'departure_delay',\n",
    "                            'departure_time', 'departure_uncertainty']]\n",
    "        kt4 = kt2.trip.apply(pd.Series)\n",
    "        kt5 = kt2.vehicle.apply(pd.Series)\n",
    "        KT_F = pd.concat([kt2, kt3_f, kt4, kt5], axis=1)\n",
    "        KT_F = KT_F[['trip_id', 'start_time', 'start_date', 'route_id', 'stop_id', 'stop_sequence', 'arrival_delay', 'arrival_time', 'arrival_uncertainty','departure_delay' , 'departure_time',  'departure_uncertainty', 'schedule_relationship', 'id', 'timestamp']]\n",
    "        KT_F = KT_F.loc[~KT_F['timestamp'].isna()]\n",
    "        KT_F = KT_F.loc[:,~KT_F.columns.duplicated()]\n",
    "        KT_F = KT_F.merge(Route_Shape_stop, on = ['stop_sequence', 'stop_id', 'route_id', 'trip_id'], how = 'left')\n",
    "        engine = create_engine('mysql+pymysql://sharma21_rw:FN9h*MhP9F!q@resmysql03.qut.edu.au/gtfs')\n",
    "        KT_F.to_sql(con=engine, name=TableName, if_exists='append')\n",
    "        KT_F.to_csv(TUdirectory3 + '/TU feed '+ date_name + ' '+ str(t)  + '.csv')\n",
    "        KT_F_2 = KT_F.copy(deep = True)\n",
    "        KT_F_copy = KT_F.copy(deep = True).loc[~KT_F['stop_sequence'].isna()][['shape_id', 'trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence']]\n",
    "        KT_F_copy['stop_sequence2'] =  KT_F_copy['stop_sequence']-1\n",
    "        KT_F_copy = KT_F_copy.merge(KT_F_copy, left_on = ['trip_id', 'stop_sequence'], right_on = ['trip_id', 'stop_sequence2'], how = 'inner')\n",
    "        KT_F_copy[['stop_sequence_x', 'stop_sequence_y']] = KT_F_copy[['stop_sequence_x', 'stop_sequence_y']].astype(int)\n",
    "        distance_file[['stop_seq_1', 'stop_seq_2']] = distance_file[['stop_seq_1', 'stop_seq_2']].astype(int)\n",
    "        distance_file['shape_id'] = distance_file['shape_id'].astype(str)\n",
    "        KT_F_copy = KT_F_copy.merge(distance_file, left_on = ['shape_id_x', 'stop_sequence_x', 'stop_sequence_y'], right_on = ['shape_id', 'stop_seq_1', 'stop_seq_2'], how = 'inner')\n",
    "        KT_F_copy['Travel_Speed_DtoA'] = 3600*KT_F_copy['distance']/(KT_F_copy['arrival_time_y'] - KT_F_copy['departure_time_x'])\n",
    "        KT_F_copy[['shape_id_x', 'trip_id', 'stop_id_x', 'stop_sequence_x', 'stop_id_y', 'stop_sequence_y','distance', 'Travel_Speed_DtoA']].to_csv(TUdirectory4 + '/TU Speed'+ date_name + ' '+ str(t)  + '.csv')\n",
    "        response.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS_Static = urllib.request.urlretrieve(gtfs_static_link, Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "zip_file = ZipFile(Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "trips = pd.read_csv(zip_file.open('trips.txt'))\n",
    "stop_times = pd.read_csv(zip_file.open('stop_times.txt'))\n",
    "shapes = pd.read_csv(zip_file.open('shapes.txt'))\n",
    "shapes['shape_pt_sequence'] = shapes['shape_pt_sequence'].astype(str)\n",
    "Route_Shape_stop = stop_times.merge(trips, on = 'trip_id', how = 'left')\n",
    "Route_Shape_stop = Route_Shape_stop[['shape_id', 'stop_id', 'stop_sequence', 'route_id', 'trip_id']].drop_duplicates(keep = 'first')\n",
    "Route_Shape_stop['stop_sequence'] = Route_Shape_stop['stop_sequence'].astype(int)\n",
    "Route_Shape_stop['stop_id'] = Route_Shape_stop['stop_id'].astype(str)\n",
    "shapes2 = shapes.copy(deep = True)\n",
    "shapes2['shape_pt_sequence_next'] = (shapes2['shape_pt_sequence'].astype(int) + 1).astype(str)\n",
    "shapes2['stop_seq_1'] = (shapes2['shape_pt_sequence'].astype(int)/10000).astype(int)\n",
    "shapes2['stop_seq_2'] = (shapes2['shape_pt_sequence_next'].astype(int)/10000).astype(int)\n",
    "shapes3 = shapes2.merge(shapes, left_on = ['shape_id', 'shape_pt_sequence_next'], right_on = ['shape_id', 'shape_pt_sequence'], how = 'left')\n",
    "shapes3['distance'] = Distance(shapes3['shape_pt_lat_x'].tolist(),shapes3['shape_pt_lon_x'].tolist(),shapes3['shape_pt_lat_y'].tolist(),shapes3['shape_pt_lon_y'].tolist())\n",
    "shapes3['distance'] = shapes3['distance']/1000\n",
    "shapes3 = shapes3.loc[shapes3['stop_seq_1'] == shapes3['stop_seq_2']]\n",
    "shapes3['stop_seq_2'] = shapes3['stop_seq_2'] + 1\n",
    "shapes4 = pd.DataFrame(shapes3.groupby(['shape_id', 'stop_seq_1', 'stop_seq_2'], as_index = False)['distance'].sum())\n",
    "distance_file = shapes4\n",
    "date_name1 = date_name\n",
    "distance_file.to_csv('distance_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
