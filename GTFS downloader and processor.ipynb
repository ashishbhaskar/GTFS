{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         GTFS real-time downnloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install gtfs-realtime-bindings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gtfs-realtime-bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install protobuf3-to-dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.transit import gtfs_realtime_pb2\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from zipfile import ZipFile\n",
    "from pyproj import Geod\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import requests\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from protobuf_to_dict import protobuf_to_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining some methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgs84_geod = Geod(ellps='WGS84') #Distance will be measured on this ellipsoid - more accurate than a spherical method\n",
    "def monthname(mydate):\n",
    "    mydate = datetime.datetime.now()\n",
    "    m = mydate.strftime(\"%B\")\n",
    "    return(m)\n",
    "\n",
    "def Brisbane(epoch):\n",
    "    a = time.strftime(\"%d-%m-%Y %H:%M:%S\", time.localtime(epoch))\n",
    "    return(a)\n",
    "\n",
    "def createCSVfile(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            k = 0\n",
    "            for item in i:  \n",
    "                f.write(str(item) + ',')\n",
    "                k = k+1\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def inputCSVfile(csvfile):\n",
    "    list1= []\n",
    "    with open(csvfile, 'r') as f:\n",
    "        for i in f:\n",
    "            j = i.split(',')\n",
    "            \n",
    "            le = len(j)\n",
    "            j[le - 1] = (j[le- 1]).strip()\n",
    "            list1.append(j)\n",
    "        return(list1)\n",
    "\n",
    "def createCSVfileWCD(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            f.write(str(i))\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def Distance(lat1,lon1,lat2,lon2):\n",
    "    az12 ,az21,dist = wgs84_geod.inv(lon1,lat1,lon2,lat2) #Yes, this order is correct\n",
    "    return (dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = r\"C:\\Users\\n10680535\\OneDrive - Queensland University of Technology\\Shubham Sharma\\Research\\2. Codes\\1. GTFS Data Downloader/\"   #ensure slash sign in the end \"/\" not \"\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sub-folders to store different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Static_Folder = Working_Directory + 'GTFS Static/'\n",
    "Realtime_folder  = Working_Directory + 'GTFS Realtime/'\n",
    "if not os.path.exists(Static_Folder):\n",
    "    os.makedirs(Static_Folder)\n",
    "if not os.path.exists(Realtime_folder):\n",
    "    os.makedirs(Realtime_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS-realtime download link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_realtime_link = r'https://gtfsrt.api.translink.com.au/Feed/SEQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS- static download link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_static_link = r\"https://gtfsrt.api.translink.com.au/GTFS/SEQ_GTFS.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and processing static and real-time feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0\n",
    "datee = datetime.datetime.strptime(str(Brisbane(t1)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "m = datee.month\n",
    "y = datee.year\n",
    "d = datee.day\n",
    "m_name = monthname(Brisbane(t1))\n",
    "date_name1 = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "index_o = 0\n",
    "while True:\n",
    "    #Directories for a new Day\n",
    "    t = int(time.time())\n",
    "    datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "    m = datee.month\n",
    "    y = datee.year\n",
    "    d = datee.day\n",
    "    m_name = monthname(Brisbane(t))\n",
    "    date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "    Month_year = str(m_name)+\" ,\"+str(y)\n",
    "    if date_name!= date_name1:\n",
    "        index_o = 0\n",
    "        datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "        m = datee.month\n",
    "        y = datee.year\n",
    "        d = datee.day\n",
    "        m_name = monthname(Brisbane(t))\n",
    "        date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "        Month_year = str(m_name)+\" ,\"+str(y)\n",
    "        TUdirectory = Realtime_folder + \"TripUpdate entity\" +\"/\" + 'TU '+str(Month_year)\n",
    "        VPdirectory = Realtime_folder + \"VehiclePosition entity\" +\"/\" + 'VP '+str(Month_year)\n",
    "        if not os.path.exists(TUdirectory):\n",
    "            os.makedirs(TUdirectory)\n",
    "        TUdirectory2= TUdirectory+ \"/\"+ 'TU '+str(date_name)\n",
    "        if not os.path.exists(TUdirectory2):\n",
    "            os.makedirs(TUdirectory2) \n",
    "        TUdirectory3= TUdirectory2+ \"/\"+ 'TU feeds '+str(date_name)\n",
    "        TUdirectory4= TUdirectory2+ \"/\"+ 'TU Speed Analysis'+str(date_name)\n",
    "        if not os.path.exists(TUdirectory3):\n",
    "            os.makedirs(TUdirectory3)\n",
    "        if not os.path.exists(TUdirectory4):\n",
    "            os.makedirs(TUdirectory4)\n",
    "        if not os.path.exists(VPdirectory):\n",
    "            os.makedirs(VPdirectory)\n",
    "        VPdirectory2= VPdirectory+ \"/\"+ 'VP '+str(date_name)\n",
    "        if not os.path.exists(VPdirectory2):\n",
    "            os.makedirs(VPdirectory2)\n",
    "            \n",
    "        #Downloading and processing static file\n",
    "        \n",
    "        GTFS_Static = urllib.request.urlretrieve(gtfs_static_link, Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "        zip_file = ZipFile(Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "        trips = pd.read_csv(zip_file.open('trips.txt'))\n",
    "        stop_times = pd.read_csv(zip_file.open('stop_times.txt'))\n",
    "        shapes = pd.read_csv(zip_file.open('shapes.txt'))\n",
    "        shapes['shape_pt_sequence'] = shapes['shape_pt_sequence'].astype(str)\n",
    "        Route_Shape_stop = stop_times.merge(trips, on = 'trip_id', how = 'left')\n",
    "        Route_Shape_stop = Route_Shape_stop[['shape_id', 'stop_id', 'stop_sequence', 'route_id', 'trip_id']].drop_duplicates(keep = 'first')\n",
    "        Route_Shape_stop['stop_sequence'] = Route_Shape_stop['stop_sequence'].astype(int)\n",
    "        Route_Shape_stop['stop_id'] = Route_Shape_stop['stop_id'].astype(str)\n",
    "        shapes2 = shapes.copy(deep = True)\n",
    "        shapes2['shape_pt_sequence_next'] = (shapes2['shape_pt_sequence'].astype(int) + 1).astype(str)\n",
    "        shapes2['stop_seq_1'] = (shapes2['shape_pt_sequence'].astype(int)/10000).astype(int)\n",
    "        shapes2['stop_seq_2'] = (shapes2['shape_pt_sequence_next'].astype(int)/10000).astype(int)\n",
    "        shapes3 = shapes2.merge(shapes, left_on = ['shape_id', 'shape_pt_sequence_next'], right_on = ['shape_id', 'shape_pt_sequence'], how = 'left')\n",
    "        shapes3['distance'] = Distance(shapes3['shape_pt_lat_x'].tolist(),shapes3['shape_pt_lon_x'].tolist(),shapes3['shape_pt_lat_y'].tolist(),shapes3['shape_pt_lon_y'].tolist())\n",
    "        shapes3['distance'] = shapes3['distance']/1000\n",
    "        shapes3 = shapes3.loc[shapes3['stop_seq_1'] == shapes3['stop_seq_2']]\n",
    "        shapes3['stop_seq_2'] = shapes3['stop_seq_2'] + 1\n",
    "        shapes4 = pd.DataFrame(shapes3.groupby(['shape_id', 'stop_seq_1', 'stop_seq_2'], as_index = False)['distance'].sum())\n",
    "        distance_file = shapes4\n",
    "        date_name1 = date_name\n",
    "        \n",
    "    #Downloading the feed\n",
    "    \n",
    "    feed = gtfs_realtime_pb2.FeedMessage()\n",
    "    response = urllib.request.urlopen('https://gtfsrt.api.translink.com.au/Feed/SEQ')\n",
    "    feed.ParseFromString(response.read())\n",
    "    dict_obj = protobuf_to_dict(feed)\n",
    "    r = json.dumps(dict_obj)\n",
    "    loaded_r = json.loads(r)\n",
    "    kppa = pd.DataFrame(loaded_r['entity'])\n",
    "    kt = kppa.trip_update.apply(pd.Series)\n",
    "    kt = pd.concat([kppa, kt], axis= 1)\n",
    "    kt2 = kt['stop_time_update'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\n",
    "    kt2 = kt.merge(kt2, left_index=True, right_index=True, how = 'left')\n",
    "    kt2.columns = [ 'id', 'trip_update', 'vehicle', 0 , 'stop_time_update', 'timestamp', 'trip','vehicle2', 'value']\n",
    "    kt3 = kt2.value.apply(pd.Series)\n",
    "    kt3_1 = kt3.arrival.apply(pd.Series)\n",
    "    kt3_1.columns = ['0', 'arrival_delay', 'arrival_time', 'arrival_uncertainty']\n",
    "    kt3_2 = kt3.departure.apply(pd.Series)\n",
    "    kt3_2.columns = ['0', 'departure_delay', 'departure_time', 'departure_uncertainty']\n",
    "    kt3_f = pd.concat([kt3, kt3_1,kt3_2], axis = 1)\n",
    "    kt4 = kt2.trip.apply(pd.Series)\n",
    "    kt5 = kt2.vehicle2.apply(pd.Series)\n",
    "    kt5.columns = [0, 'vehicle_id']\n",
    "    KT_F = pd.concat([kt2, kt3_f, kt4, kt5], axis=1)\n",
    "    KT_F = KT_F[['trip_id', 'start_time', 'start_date', 'route_id', 'stop_id', 'stop_sequence', 'arrival_delay', 'arrival_time', 'arrival_uncertainty','departure_delay' , 'departure_time',  'departure_uncertainty', 'schedule_relationship', 'vehicle_id', 'timestamp']]\n",
    "    KT_F = KT_F.loc[~KT_F['timestamp'].isna()]\n",
    "    KT_F = KT_F.loc[:,~KT_F.columns.duplicated()]\n",
    "    KT_F = KT_F.merge(Route_Shape_stop, on = ['stop_sequence', 'stop_id', 'route_id', 'trip_id'], how = 'left')\n",
    "    KT_F.to_csv(TUdirectory3 + '/TU feed'+ date_name + ' '+ str(t)  + '.csv')\n",
    "    KT_F_copy = KT_F.copy(deep = True).loc[~KT_F['stop_sequence'].isna()][['shape_id', 'trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence']]\n",
    "    KT_F_copy['stop_sequence2'] =  KT_F_copy['stop_sequence']-1\n",
    "    KT_F_copy = KT_F_copy.merge(KT_F_copy, left_on = ['trip_id', 'stop_sequence'], right_on = ['trip_id', 'stop_sequence2'], how = 'inner')\n",
    "    KT_F_copy[['stop_sequence_x', 'stop_sequence_y']] = KT_F_copy[['stop_sequence_x', 'stop_sequence_y']].astype(int)\n",
    "    distance_file[['stop_seq_1', 'stop_seq_2']] = distance_file[['stop_seq_1', 'stop_seq_2']].astype(int)\n",
    "    #distance_file['shape_id'] = distance_file['shape_id'].astype(str)\n",
    "    KT_F_copy = KT_F_copy.merge(distance_file, left_on = ['shape_id_x', 'stop_sequence_x', 'stop_sequence_y'], right_on = ['shape_id', 'stop_seq_1', 'stop_seq_2'], how = 'inner')\n",
    "    KT_F_copy['Travel_Speed_DtoA'] = 3600*KT_F_copy['distance']/(KT_F_copy['arrival_time_y'] - KT_F_copy['departure_time_x'])\n",
    "    KT_F_copy[['shape_id_x', 'trip_id', 'stop_id_x', 'stop_sequence_x', 'stop_id_y', 'stop_sequence_y','distance', 'Travel_Speed_DtoA']].to_csv(TUdirectory4 + '/TU Speed'+ date_name + ' '+ str(t)  + '.csv')\n",
    "    vt = kppa.vehicle.apply(pd.Series)\n",
    "    vt1 = vt.position.apply(pd.Series)\n",
    "    vt2 = vt.trip.apply(pd.Series)\n",
    "    vt4 = vt.vehicle.apply(pd.Series)\n",
    "    VE = pd.concat([vt, vt1, vt2, vt4], axis=1)\n",
    "    VE = VE[['current_status', 'stop_id', 'timestamp', 'trip_id','latitude', 'longitude' , 'route_id','id', 'label' ]]\n",
    "    VE = VE.loc[~VE['timestamp'].isna()]\n",
    "    VE.to_csv(VPdirectory2 + '/VP feed '+ date_name + ' '+ str(t)  + '.csv')\n",
    "    index_o = index_o + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
