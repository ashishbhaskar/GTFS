{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.transit import gtfs_realtime_pb2\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from zipfile import ZipFile\n",
    "from pyproj import Geod\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import requests\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from protobuf_to_dict import protobuf_to_dict\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io import sql\n",
    "from pyproj import Geod\n",
    "from gtfs_functions import speed_trip_trajectory_preprocessing_analysis\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgs84_geod = Geod(ellps='WGS84') #Distance will be measured on this ellipsoid - more accurate than a spherical method\n",
    "def monthname(mydate):\n",
    "    ##Generate the today's month name instead of the selected date\n",
    "    #mydate = datetime.datetime.now()\n",
    "    m = mydate.strftime(\"%B\")\n",
    "    return(m)\n",
    "\n",
    "def Brisbane(epoch):\n",
    "    a = time.strftime(\"%d-%m-%Y %H:%M:%S\", time.localtime(epoch))\n",
    "    return(a)\n",
    "\n",
    "def createCSVfile(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            k = 0\n",
    "            for item in i:  \n",
    "                f.write(str(item) + ',')\n",
    "                k = k+1\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def inputCSVfile(csvfile):\n",
    "    list1= []\n",
    "    with open(csvfile, 'r') as f:\n",
    "        for i in f:\n",
    "            j = i.split(',')\n",
    "            \n",
    "            le = len(j)\n",
    "            j[le - 1] = (j[le- 1]).strip()\n",
    "            list1.append(j)\n",
    "        return(list1)\n",
    "\n",
    "def createCSVfileWCD(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            f.write(str(i))\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def Distance(lat1,lon1,lat2,lon2):\n",
    "    az12 ,az21,dist = wgs84_geod.inv(lon1,lat1,lon2,lat2) #Yes, this order is correct\n",
    "    return (dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a59929",
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = r\"Y:\\\\Data\\\\GTFS_NEW\\\\\"\n",
    "Static_Folder = Working_Directory + os.sep + 'GTFS Static/'\n",
    "Realtime_folder  = Working_Directory+ os.sep + 'GTFS Realtime/'\n",
    "Traj_folder = Realtime_folder + os.sep + 'VehicleTrajectory entity/'\n",
    "if not os.path.exists(Static_Folder):\n",
    "    os.makedirs(Static_Folder)\n",
    "if not os.path.exists(Realtime_folder):\n",
    "    os.makedirs(Realtime_folder)\n",
    "if not os.path.exists(Traj_folder):\n",
    "    os.makedirs(Traj_folder)\n",
    "    \n",
    "gtfs_static_link = r\"https://gtfsrt.api.translink.com.au/GTFS/SEQ_GTFS.zip\"\n",
    "gtfs_realtime_link = r'https://gtfsrt.api.translink.com.au/api/realtime/SEQ/VehiclePositions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0\n",
    "datee = datetime.datetime.strptime(str(Brisbane(t1)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "m = datee.month\n",
    "y = datee.year\n",
    "d = datee.day\n",
    "m_name = monthname(datee)\n",
    "date_name1 = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "TableName = str(m_name)+\"_\"+str(y)\n",
    "TableName = TableName.lower()\n",
    "index_o = 0\n",
    "curr_date = date_name1\n",
    "########################Between 12:30am to 4:30am Operating Hours#############################\n",
    "end_time = datetime.time(0,30,0)\n",
    "start_time = datetime.time(4,30,0)\n",
    "i = 1\n",
    "flags = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f7212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        clear_output(wait=True)\n",
    "        #Directories for a new Day\n",
    "        t = int(time.time())\n",
    "        datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "        m = datee.month\n",
    "        y = datee.year\n",
    "        d = datee.day\n",
    "        #m_name = monthname(Brisbane(t))\n",
    "        m_name = monthname(datee)\n",
    "        date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "        Month_year = str(m_name)+\" ,\"+str(y)\n",
    "        if date_name!= date_name1:\n",
    "            index_o = 0\n",
    "            datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "            m = datee.month\n",
    "            y = datee.year\n",
    "            d = datee.day\n",
    "            #m_name = monthname(Brisbane(t))\n",
    "            m_name = monthname(datee)\n",
    "            date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "            TableName = str(m_name)+\"_\"+str(y)\n",
    "            TableName = TableName.lower()\n",
    "            Month_year = str(m_name)+\" ,\"+str(y)\n",
    "            TUdirectory = Realtime_folder + \"TripUpdate entity\" +\"/\" + 'TU '+str(Month_year)\n",
    "            VPdirectory = Realtime_folder + \"VehiclePosition entity\" +\"/\" + 'VP '+str(Month_year)\n",
    "            if not os.path.exists(TUdirectory):\n",
    "                os.makedirs(TUdirectory)\n",
    "            TUdirectory2= TUdirectory+ \"/\"+ 'TU '+str(date_name)\n",
    "            if not os.path.exists(TUdirectory2):\n",
    "                os.makedirs(TUdirectory2) \n",
    "            TUdirectory3= TUdirectory2+ \"/\"+ 'TU feeds '+str(date_name)\n",
    "            TUdirectory4= TUdirectory2+ \"/\"+ 'TU Speed Analysis'+str(date_name)\n",
    "            if not os.path.exists(TUdirectory3):\n",
    "                os.makedirs(TUdirectory3)\n",
    "            if not os.path.exists(TUdirectory4):\n",
    "                os.makedirs(TUdirectory4)\n",
    "            if not os.path.exists(VPdirectory):\n",
    "                os.makedirs(VPdirectory)\n",
    "            VPdirectory2= VPdirectory+ \"/\"+ 'VP '+str(date_name)\n",
    "            if not os.path.exists(VPdirectory2):\n",
    "                os.makedirs(VPdirectory2)\n",
    "\n",
    "            #Downloading and processing static file\n",
    "\n",
    "            GTFS_Static = urllib.request.urlretrieve(gtfs_static_link, Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "            zip_file = ZipFile(Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "            trips = pd.read_csv(zip_file.open('trips.txt'))\n",
    "            stop_times = pd.read_csv(zip_file.open('stop_times.txt'))\n",
    "            shapes = pd.read_csv(zip_file.open('shapes.txt'), low_memory=False)\n",
    "            shapes['shape_pt_sequence'] = shapes['shape_pt_sequence'].astype(str)\n",
    "            Route_Shape_stop = stop_times.merge(trips, on = 'trip_id', how = 'left')\n",
    "            Route_Shape_stop = Route_Shape_stop[['shape_id', 'stop_id', 'stop_sequence', 'route_id', 'trip_id']].drop_duplicates(keep = 'first')\n",
    "            Route_Shape_stop['stop_sequence'] = Route_Shape_stop['stop_sequence'].astype(int)\n",
    "            Route_Shape_stop['stop_id'] = Route_Shape_stop['stop_id'].astype(str)\n",
    "            shapes2 = shapes.copy(deep = True)\n",
    "            shapes2['shape_pt_sequence_next'] = (shapes2['shape_pt_sequence'].astype(int) + 1).astype(str)\n",
    "            shapes2['stop_seq_1'] = (shapes2['shape_pt_sequence'].astype(int)/10000).astype(int)\n",
    "            shapes2['stop_seq_2'] = (shapes2['shape_pt_sequence_next'].astype(int)/10000).astype(int)\n",
    "            shapes3 = shapes2.merge(shapes, left_on = ['shape_id', 'shape_pt_sequence_next'], right_on = ['shape_id', 'shape_pt_sequence'], how = 'left')\n",
    "            shapes3['distance'] = Distance(shapes3['shape_pt_lat_x'].tolist(),shapes3['shape_pt_lon_x'].tolist(),shapes3['shape_pt_lat_y'].tolist(),shapes3['shape_pt_lon_y'].tolist())\n",
    "            shapes3['distance'] = shapes3['distance']/1000\n",
    "            shapes3 = shapes3.loc[shapes3['stop_seq_1'] == shapes3['stop_seq_2']]\n",
    "            shapes3['stop_seq_2'] = shapes3['stop_seq_2'] + 1\n",
    "            shapes4 = pd.DataFrame(shapes3.groupby(['shape_id', 'stop_seq_1', 'stop_seq_2'], as_index = False)['distance'].sum())\n",
    "            distance_file = shapes4\n",
    "            date_name1 = date_name\n",
    "                  \n",
    "        if date_name1 == date_name and curr_date != date_name and end_time <= datee.time() < start_time:\n",
    "            clear_output(wait=True)\n",
    "            Trajdirectory = Traj_folder + \"Traj \" + str(Month_year)\n",
    "            if not os.path.exists(Trajdirectory):\n",
    "                os.makedirs(Trajdirectory)\n",
    "            try:\n",
    "                timer_st = time.time()\n",
    "                print(\"processing date = \", date_name)\n",
    "                previous_datee = datee + timedelta(-i)\n",
    "                prev_date_name = str(previous_datee.day)+\"-\"+str(previous_datee.month)+\"-\"+str(previous_datee.year)\n",
    "                prev_m_name = monthname(previous_datee)\n",
    "                prev_table_name = str(prev_m_name)+\"_\"+str(previous_datee.year)\n",
    "                prev_table_name = prev_table_name.lower()\n",
    "                print(f\"processing {prev_date_name}....\")\n",
    "                _, _, df_speed_traj = speed_trip_trajectory_preprocessing_analysis(previous_datee)\n",
    "                print(\"processing df_speed trajectory done!\")\n",
    "                print(df_speed_traj.shape)\n",
    "                timer_end = time.time()\n",
    "                print(f\"The duration it from vp preprocessing = {(timer_end - timer_st)/60} mins\")\n",
    "                prev_table_name = prev_table_name + \"_traj\"\n",
    "                df_speed_traj.to_csv(f\"{Trajdirectory}/trip_trajectory_{prev_date_name}.csv\")\n",
    "                curr_date = date_name\n",
    "            except Exception as e:\n",
    "                print(\"an exception occurred\")\n",
    "                print(str(e))\n",
    "#                 break\n",
    "#                 pass\n",
    "        #Downloading the feed\n",
    "\n",
    "        feed = gtfs_realtime_pb2.FeedMessage()\n",
    "        response = urllib.request.urlopen(gtfs_realtime_link)\n",
    "        feed.ParseFromString(response.read())\n",
    "        dict_obj = protobuf_to_dict(feed)\n",
    "        r = json.dumps(dict_obj)\n",
    "        loaded_r = json.loads(r)\n",
    "        if flags ==1:\n",
    "            kppa = pd.DataFrame(loaded_r['entity'])\n",
    "            if kppa.shape != kppa1.shape:\n",
    "                vt = kppa.vehicle.apply(pd.Series)\n",
    "                vt1 = vt.position.apply(pd.Series)\n",
    "                vt2 = vt.trip.apply(pd.Series)\n",
    "                vt4 = vt.vehicle.apply(pd.Series)\n",
    "                VE = pd.concat([vt, vt1, vt2, vt4], axis=1)\n",
    "                VE = VE[['current_status', 'stop_id', 'timestamp', 'trip_id','latitude', 'longitude' , 'route_id','id', 'label' ]]\n",
    "                VE = VE.loc[~VE['timestamp'].isna()]\n",
    "                VE.to_csv(VPdirectory2 + '/VP feed '+ date_name + ' '+ str(t)  + '.csv')\n",
    "                index_o = index_o + 1\n",
    "                kppa1 = kppa.copy(deep = True)\n",
    "                flags = 1\n",
    "        if flags == 0:\n",
    "            kppa = pd.DataFrame(loaded_r['entity'])\n",
    "            vt = kppa.vehicle.apply(pd.Series)\n",
    "            vt1 = vt.position.apply(pd.Series)\n",
    "            vt2 = vt.trip.apply(pd.Series)\n",
    "            vt4 = vt.vehicle.apply(pd.Series)\n",
    "            VE = pd.concat([vt, vt1, vt2, vt4], axis=1)\n",
    "            VE = VE[['current_status', 'stop_id', 'timestamp', 'trip_id','latitude', 'longitude' , 'route_id','id', 'label' ]]\n",
    "            VE = VE.loc[~VE['timestamp'].isna()]\n",
    "            VE.to_csv(VPdirectory2 + '/VP feed '+ date_name + ' '+ str(t)  + '.csv')\n",
    "            index_o = index_o + 1\n",
    "            kppa1 = kppa.copy(deep = True)\n",
    "            flags = 1\n",
    "        response.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac35c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97e16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5b7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
